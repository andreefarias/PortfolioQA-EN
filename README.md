# Portfolio QA - Andr√© Farias [EN]

###### Welcome! üéâ
This document aims to showcase some of my key skills in the field of software testing, automation, bug management, and testing.

#
# üì¢ Introduction:
I have been working in the Quality Assurance (QA) field for over two years, a choice that perfectly aligns with my constant pursuit of excellence.

I entered the QA profession through a unique internship opportunity at the renowned global company called Liferay. I admit that initially, I had limited knowledge in the field, but it was love at first sight. In just three months of dedicated work in test automation, I excelled and became part of a team specialized in low-code applications focused on object-oriented programming.

üöÄüöÄüöÄ

Facing the challenge of being one of the first QAs embedded in a globally scaled and highly usable team represented a significant leap in my career. I was exposed to a wide range of activities, starting with test analysis and correction, release analysis, bug management, and evolving into more complex responsibilities such as story validation, PR reviews, and mentoring new team members.

As a highlight, to optimize our test suites, I incorporated the use of APIs in test creation, providing an agile, efficient, and effective approach to ensuring software quality. Additionally, I had the enriching opportunity to lead a small squad, where my mission was to assist testers in engaging with user stories and supporting them in routine activities, fostering greater autonomy. The responsibility gave me more autonomy to interact more frequently with developers and product owners.

During this period, I significantly expanded my knowledge in various testing modalities, covering acceptance, stress, regression, system, usability, and upgrade testing. Furthermore, I gained experience in conducting tests on a variety of database systems, including MySQL, MariaDB, OracleSQL, and PostgreSQL.

#
# üíª Tests:
### Acceptance Testing:
###### Working in a continuous development context, I participated in numerous user stories across different applications. In acceptance testing, my goal was to ensure that what was defined in meetings with product owners and designers was being implemented.

### Stress Testing:
###### Due to different types of clients with high scalability, we occasionally subjected our applications to stress testing, creating extreme scenarios to assess how the system behaved under intense pressure.

### Regression Testing:
###### Contextualized with the new modifications, we explored scenarios in which errors could occur. Additionally, we analyzed our tests, identifying failures that arose during the incorporation of the new code.

### System Testing:
###### We validated integrated scenarios involving all internal and external components, executing real-use cases and simulating interactions like a common user.

### Usability Testing:
###### With a focus on practicality, we conducted tests on our product, sharing our experiences.

### Upgrade Testing:
###### We verified the integrity of our applications after performing upgrades, ensuring that no data losses occurred.
###### üìå Example of an upgrade bug: [Ticket](https://liferay.atlassian.net/browse/LPS-192174)

#
# ‚≠ê Skills:
### Analyzing releases:
###### Test analysis and manual verification of new product updates.

### Automate functional tests:
###### Writing new test cases using the Poshi language, which we became familiar with upon joining Liferay. It is straightforward as it is based on Java, JavaScript, and Selenium.
###### üìå Example of automation: [#1289](https://github.com/liferay-bpm-qa/liferay-portal/pull/1289)

### Assist new team members:
###### I'd the rewarding opportunity to mentor young individuals in their early careers in the field. This significant challenge not only strengthened my confidence but also contributed to personal development, especially in the aspect of communication.

### Create test scenarios:
###### Before testing user stories, we created a comprehensive test list to cover various scenarios. Additionally, we explored the functionalities, ensuring complete coverage during the testing process.

### Discuss acceptance criteria:
###### QA meetings to discuss acceptance criteria, aligning all expected aspects, followed by a discussion with the product team about the user story.

### Document story validations:
###### Upon completing the validation of the story, we created documentation highlighting key events and relevant cases covered. Additionally, we listed all identified bugs, thereby contributing to the product team's understanding of how they were addressed by the QA team.

### Explore APIs:
###### APIs were our allies in various instances, saving significant time. During story testing, we allocated dedicated time to evaluate them. In certain scenarios, we developed automated tests, contributing significantly to reducing the impact on servers, especially in functional tests, which have a higher cost.
###### üìå Example of API structure: [#1115](https://github.com/liferay-bpm-qa/liferay-portal/pull/1115)

### Fix test cases:
###### In some situations, it was necessary to perform maintenance on our tests. In this context, we fixed or refactored broken or obsolete tests.
###### üìå Example of test fix: [#1307](https://github.com/liferay-bpm-qa/liferay-portal/pull/1307)

### Interact with Developers and Product:
###### Naturally, I became more involved with the development team and the product team, seeking more information about the product and expressing some personal opinions.

### Investigate and report bugs:
###### We analyzed scenarios provided by clients and consultants to identify flaws in our systems. If confirmed, we proceeded with bug reports, followed by determining their priority. In the vast majority of cases, bugs were already identified by the QA team.
###### üìå Example of a bug involving accessibility: [Ticket](https://liferay.atlassian.net/browse/LPS-201938)

### Review pull requests:
###### I reviewed PRs from my mini-squad and new team members, assisting with better commits, logic, and scenarios.

### Manually test:
###### We conducted exploratory tests on components and user stories, occasionally performing smoke tests to prioritize essential functionalities.

### Validate stories:
###### Weekly process in a continuous development and improvement of product applications.

### Verify testing routines:
###### Daily analyses to identify potential bugs or blockers found during tests.

#
# üîç Testing tools:
### Poshi + Testray (Java, JavaScript e Selenium):
###### Poshi: company's private test automation tool based on Java, JavaScript, and Selenium. With prior knowledge of Java, it was quite easy to learn how to use it.
###### Testray: private system focused on displaying our test reports, integrated with Jenkins.

### Git e GitHub:
###### We use them to version, review, and manage our development code.

### Docker:
###### Program that assists us by virtualizing some applications, especially in the use of different databases.

### API Explorer / Postman:
###### API Explorer: was a tool that helped us manage the endpoints of the portal.
###### Postman: similar process to API Explorer, being more robust. Used a few occasions.
###### üìå Example of API structure: [#1115](https://github.com/liferay-bpm-qa/liferay-portal/pull/1115)

### Jenkins:
###### Platform I used to check regression bugs and analyze logs.

#
# üìù Test management:
### Jira:
###### Commonly used to organize development processes, a robust project management and issue tracking platform. Through Jira, we reported bugs, and in critical cases, we always shared via Slack, our internal communication platform.

### Google Sheets:
###### We used spreadsheets to create our test scenarios.

### Kanban e Scrum:
###### These agile methodologies were routinely addressed, with visualization done through the Jira platform.

### BDD (Behavior Driven Development):
###### We wrote behavior-driven test cases.
###### üìå Example of using BDD: [Ticket](https://liferay.atlassian.net/browse/LPS-202175)

#
# üìÇ Databases:
### MySQL:
###### The primary database used was MySQL, which assisted us in data analysis through queries on tables. I used MySQL Workbench as a tool to facilitate this process.

### MariaDB, OracleSQL, PostgreSQL, SQL Server:
###### Occasionally, I turned to other options such as MariaDB, OracleSQL, PostgreSQL, SQL Server to identify specific bugs.

### DBeaver:
###### To check tables in different MySQL databases, I used DBeaver, which is a database manager.

#
# üìû Contact:
###### <img align="center" title="LinkedIn" height="20" width="30" src="https://www.svgrepo.com/show/448234/linkedin.svg"> [LinkedIn](https://www.linkedin.com/in/andreefarias/)
###### <img align="center" title="Outlook" height="20" width="30" src="https://www.svgrepo.com/show/373951/outlook.svg"> [Hotmail](mailto:andre_luiz_08@hotmail.com)
